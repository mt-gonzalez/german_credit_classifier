{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \n",
    "First I will explore the dataset to gain an insight which will help to define the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/raw.csv\", sep=\" \") # The file use space to separate columns\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns names are not defined in the csv file, so we change them in the file reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = [\"Status\", \"Duration\", \"History\", \"Purpose\", \"Amount\", \"Savings\", \"Employment\", \"Installment_Rate\", \n",
    "                 \"Personal_Status\", \"Other_Debtors\", \"Residence_Years\", \"Property\", \"Age\", \"Other_Installments\", \"Housing\",\n",
    "                 \"Existing_Credits\", \"Job\", \"Num_People_Liable\", \"Telephone\", \"Foreign_Worker\", \"Target\"]\n",
    "\n",
    "df = pd.read_csv(\"data/raw.csv\", sep=\" \", header=None, names=columns_names)\n",
    "print(list(df.columns))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "df_encoded = df_encoded.astype(int)\n",
    "df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_encoded.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Target\"\n",
    "columns = [col for col in df_encoded.columns if col != target_column] # Get all columns names except Target one\n",
    "columns.append(target_column)\n",
    "\n",
    "df_encoded = df_encoded[columns]\n",
    "print(list(df_encoded.columns))\n",
    "\n",
    "df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because it is a pretty clean dataset, we are ready with the pre-processing of the data and can move on to the training. But first, I would like to display some plots that are going to make the data more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize =(18, 6))\n",
    "\n",
    "# First subplot\n",
    "sns.countplot(x=\"Target\", data=df_encoded, ax=axes[0])\n",
    "axes[0].set_title(\"Target Variable Distribution\")\n",
    "\n",
    "# Second subplot\n",
    "corr_matrix = df_encoded.corr()\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", ax=axes[1])\n",
    "axes[1].set_title(\"Correlation Heatmap\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the plots above we can infer two main things** \n",
    "- The first one is that the data set is a bit **unbalanced** towards good clients. But we have the solution to this already, which is the **cost matrix** that comes asociated to the dataset, changing the weights in the decision. It will be used later in the training\n",
    "- A second observation is from the correlation matrix. This tells us that the correlation between variables is almost 0 for every one, but because we are first using decision trees this is not noticeable, although we could check this with the permutation importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"data/pre_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german_credit_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
